{
  "$schema": "https://openclaw.ai/schema/config.json",
  "version": "1.0",
  
  "agents": {
    "defaults": {
      "heartbeat": {
        "enabled": false,
        "every": "60m",
        "model": "gemini-2.0-flash-lite"
      },
      "session": {
        "contextTokens": 32000,
        "maxInputTokens": 8000,
        "pruneAfterMessages": 50,
        "autoSummarizeAfter": 30
      },
      "model": {
        "default": "gemini-2.0-flash",
        "thinking": false,
        "temperature": 0.7,
        "maxOutputTokens": 2048
      }
    }
  },

  "compaction": {
    "mode": "safeguard",
    "threshold": 0.8,
    "strategy": "summarize_old"
  },

  "streamMode": "partial",

  "skills": {
    "nodeManager": "bun"
  },

  "models": {
    "aliases": {
      "fast": "gemini-2.0-flash-lite",
      "balanced": "gemini-2.0-flash",
      "smart": "gemini-2.0-pro"
    },
    "routing": {
      "simple_queries": "fast",
      "code_tasks": "balanced",
      "complex_reasoning": "smart"
    },
    "params": {
      "gemini-2.0-flash-lite": {
        "temperature": 0.5,
        "maxOutputTokens": 1024,
        "thinking": false
      },
      "gemini-2.0-flash": {
        "temperature": 0.7,
        "maxOutputTokens": 2048,
        "thinking": false
      },
      "gemini-2.0-pro": {
        "temperature": 0.8,
        "maxOutputTokens": 4096,
        "thinking": false
      }
    }
  },

  "rateLimit": {
    "tokensPerMinute": 900000,
    "requestsPerMinute": 50,
    "dailyTokenBudget": 50000000,
    "warningThreshold": 0.8
  },

  "caching": {
    "enabled": true,
    "ttlMinutes": 30,
    "maxEntries": 1000
  },

  "tools": {
    "outputTruncation": {
      "enabled": true,
      "maxChars": 5000,
      "truncationMessage": "... [output truncated for token efficiency]"
    }
  },

  "telegram": {
    "dmPolicy": "pairing",
    "maxMessageLength": 4000
  },

  "logging": {
    "level": "info",
    "tokenUsage": true
  }
}
